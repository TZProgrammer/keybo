\subsection{Data Processing}
Two datasets are used to estimate the typing time. The first dataset, the iWeb corpus \citep{iweb} is one of the largest available collections of English text and uses a systematic selection of websites to ensure high quality data. This corpus facilitates frequency analysis of characters and character sequences. The second, the 136M Keystrokes dataset from Aalto University \citep{dhakal2018observations}, consists of typing test performance data, totaling approximately 8,228 hours from 168,000 participants. Participant metadata enables a controlled analysis of four layouts: AZERTY, Dvorak, QWERTY, and QWERTZ.


To ensure high-quality analysis of the keystroke data, several preprocessing steps are performed. First, the data is normalized by segmenting it into sessions and users, while correcting occasional errors in the source, leading to minimal data loss. Next, keystroke accuracy is assessed by applying approximate string matching between the target and typed text. This approach enables the identification and removal of any keystrokes caused by typing errors.


%  Pairs of shift keys and their resulting characters or symbols are consolidated into single units.
\begin{table}[h]
\caption{Types of Error Handled by Approximate String Matching}
\begin{center}
\begin{tabular}{l|l}
Typing Error & String typed \\ \hline
None & But thank you for the offer \\
Insertion      & But thank\underline{l} you for the offer                     \\
Deletion       & But tha\underline{\hspace{0.2em}}k you for the offer                       \\
Substitution   & But tha\underline{b}k you for the offer                      \\
\end{tabular}
\end{center}
\label{fig:typing_errors}
\end{table}

\noindent Three common typing errors are accounted for: insertion, deletion, and substitution \citep{navarro2001guided}, as illustrated in Table ~\ref{fig:typing_errors}. A keystroke validity record is generated for each typing test session. This record is updated dynamically on a per-window basis, with each window being a section of text up to some error or key input that requires processing. When a user uses arrow keys or backspaces, the correctness of the current window is assessed, added to the keystroke validity record, and a new window starts at the most recent navigation action. This procedure repeats until the last keystroke, at which point any remaining input data is included in the record. The validity record helps identify the location of errors for further processing. While insertion and substitution errors are relatively straightforward to process, deletion errors require special handling because a keystroke resulting in a deletion error can be both correct and incorrect. For example, if a user intends to type "there" but types "tere," the "e" is incorrect in its position but effectively completes the remaining substring "ere." The analysis accounts for this duality, ensuring that the focus is solely on unaffected keystroke sequences.
% Deletion errors are retained in the analysis but handled with this special consideration.

\begin{table}[h]
\caption{Top 5 Bigrams and Trigrams Extracted from the Corpus}
\begin{center}
\begin{tabular}{lllll}
\textbf{Bigram} & \textbf{Occurences} &  & \textbf{Trigram} & \textbf{Occurences} \\
th              & 9709171             &  & the              & 6076523             \\
he              & 8552661             &  & ing              & 3227179             \\
in              & 7913861             &  & and              & 2998065             \\
an              & 6389345             &  & ion              & 1716878             \\
er              & 6348583             &  & ent              & 1519196            
\end{tabular}
\end{center}
\label{fig:ngrams}
\end{table}

\noindent After preprocessing, a sliding window decomposes both the corpus and keystroke data. Character sequences from the corpus decomposition are referred to as ngrams. Bigrams and trigrams represent ngrams of lengths two and three, respectively. Analogously, sequences from the keystroke data are referred to as nstrokes, with bistrokes and tristrokes denoting lengths two and three. Each ngram encountered in the corpus is stored alongside its frequency of occurrence. Each nstroke, excluding those made in errors, is recorded as a tuple comprising its characters and a positional vector indicating the keys used. These tuples serve as identifiers for storing the typing times of unique nstroke instances within the dataset. For example, the identifier for the tristroke "the" on AZERTY, QWERTY, and QWERTZ keyboards would be ((-1, 3), (1, 2), (-3, 3), 'the'). This ensures that typing durations for each unique pattern instance, regardless of the layout they occur on, are cataloged and merged. Outliers in recorded typing times are removed using the interquartile range (IQR), and the remaining times are averaged within specific words per minute (WPM) ranges. These average typing times are later used to fine-tune the cost functions.
\input{figures/keymap}

% To-do: insert trigram frequency vs. speed graph

% To-do: insert graphics about the distribution of ngrams over the corpus (a simple bar graph should do)

% To-do: describe sequence categories identified in analysis, graph them in later sections to show that they're statistically more slow.

% To-do: describe curve fitting formula (Levenberg-Marquardt algorithm) and the associated function
